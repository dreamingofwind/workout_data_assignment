{"name":"Workout data assignment","tagline":"","body":"\r\n<html>\r\n\r\n<body>\r\n\r\n<h1>Are You Working Out Properly?</h1>\r\n\r\n<p><b>Executive Summary:</b> We are building models to predict whether someone conducting a workout is doing it properly (category A), or improperly (categories B,C,D,E). We tried 2 classification models: random forest and support vector machines. Turns out that random forest (without boosting) already generates low out of sample error. SVM does not improve performance. For the 20 predictions, both models agreed, so stacking the two models would not improve performance in this specific 20-case example. Both models on their own got 100% of the 20-case sample correct.</p>\r\n\r\n<p>Start by loading the training data. Since we want to do cross-validation, we will separate it into training and testing sets.</p>\r\n\r\n<div class=\"chunk\" id=\"unnamed-chunk-1\"><div class=\"rcode\"><div class=\"source\"><pre class=\"knitr r\"><span class=\"hl kwd\">library</span><span class=\"hl std\">(caret)</span>\r\n</pre></div>\r\n<div class=\"message\"><pre class=\"knitr r\">## Loading required package: lattice\r\n## Loading required package: ggplot2\r\n</pre></div>\r\n<div class=\"source\"><pre class=\"knitr r\"><span class=\"hl kwd\">library</span><span class=\"hl std\">(stats)</span>\r\n<span class=\"hl kwd\">library</span><span class=\"hl std\">(randomForest)</span>\r\n</pre></div>\r\n<div class=\"message\"><pre class=\"knitr r\">## randomForest 4.6-10\r\n## Type rfNews() to see new features/changes/bug fixes.\r\n</pre></div>\r\n<div class=\"source\"><pre class=\"knitr r\"><span class=\"hl kwd\">library</span><span class=\"hl std\">(e1071)</span>\r\n<span class=\"hl std\">dat</span><span class=\"hl kwb\">&lt;-</span><span class=\"hl kwd\">read.csv</span><span class=\"hl std\">(</span><span class=\"hl str\">&quot;C:/users/wjc/downloads/pml-training.csv&quot;</span><span class=\"hl std\">)</span>\r\n<span class=\"hl std\">pred</span><span class=\"hl kwb\">=</span><span class=\"hl kwd\">cbind</span><span class=\"hl std\">(dat[</span><span class=\"hl num\">8</span><span class=\"hl opt\">:</span><span class=\"hl num\">11</span><span class=\"hl std\">],dat[</span><span class=\"hl num\">37</span><span class=\"hl opt\">:</span><span class=\"hl num\">49</span><span class=\"hl std\">],dat[</span><span class=\"hl num\">60</span><span class=\"hl opt\">:</span><span class=\"hl num\">68</span><span class=\"hl std\">],dat[</span><span class=\"hl num\">84</span><span class=\"hl opt\">:</span><span class=\"hl num\">86</span><span class=\"hl std\">],dat[</span><span class=\"hl num\">102</span><span class=\"hl std\">],dat[</span><span class=\"hl num\">113</span><span class=\"hl opt\">:</span><span class=\"hl num\">124</span><span class=\"hl std\">],dat[</span><span class=\"hl num\">140</span><span class=\"hl std\">],dat[</span><span class=\"hl num\">151</span><span class=\"hl opt\">:</span><span class=\"hl num\">159</span><span class=\"hl std\">])</span>\r\n<span class=\"hl std\">class</span><span class=\"hl kwb\">=</span><span class=\"hl std\">dat</span><span class=\"hl opt\">$</span><span class=\"hl std\">classe</span>\r\n<span class=\"hl std\">pred</span><span class=\"hl kwb\">&lt;-</span><span class=\"hl kwd\">cbind</span><span class=\"hl std\">(pred,class)</span>\r\n\r\n<span class=\"hl std\">trainIndex</span><span class=\"hl kwb\">&lt;-</span><span class=\"hl kwd\">createDataPartition</span><span class=\"hl std\">(pred</span><span class=\"hl opt\">$</span><span class=\"hl std\">class,</span><span class=\"hl kwc\">p</span><span class=\"hl std\">=</span><span class=\"hl num\">0.8</span><span class=\"hl std\">,</span><span class=\"hl kwc\">list</span><span class=\"hl std\">=</span><span class=\"hl num\">FALSE</span><span class=\"hl std\">)</span>\r\n<span class=\"hl std\">training</span><span class=\"hl kwb\">&lt;-</span><span class=\"hl std\">pred[trainIndex,]</span>\r\n<span class=\"hl std\">testing</span><span class=\"hl kwb\">&lt;-</span><span class=\"hl std\">pred[</span><span class=\"hl opt\">-</span><span class=\"hl std\">trainIndex,]</span>\r\n</pre></div>\r\n</div></div>\r\n\r\n<p>Let's start with a random forest model. Build the model on training data set, then calculate accuracy on testing data set.</p>\r\n\r\n<div class=\"chunk\" id=\"unnamed-chunk-2\"><div class=\"rcode\"><div class=\"source\"><pre class=\"knitr r\"><span class=\"hl std\">rf1</span><span class=\"hl kwb\">&lt;-</span><span class=\"hl kwd\">randomForest</span><span class=\"hl std\">(class</span><span class=\"hl opt\">~</span><span class=\"hl std\">.,</span><span class=\"hl kwc\">data</span><span class=\"hl std\">=training,</span><span class=\"hl kwc\">ntree</span><span class=\"hl std\">=</span><span class=\"hl num\">100</span><span class=\"hl std\">)</span>\r\n<span class=\"hl kwd\">plot</span><span class=\"hl std\">(rf1)</span>\r\n</pre></div>\r\n</div><div class=\"rimage default\"><img src=\"figure/unnamed-chunk-2.png\" title=\"plot of chunk unnamed-chunk-2\" alt=\"plot of chunk unnamed-chunk-2\" class=\"plot\" /></div><div class=\"rcode\">\r\n<div class=\"source\"><pre class=\"knitr r\"><span class=\"hl kwd\">confusionMatrix</span><span class=\"hl std\">(</span><span class=\"hl kwd\">predict</span><span class=\"hl std\">(rf1,testing),testing</span><span class=\"hl opt\">$</span><span class=\"hl std\">class)</span>\r\n</pre></div>\r\n<div class=\"output\"><pre class=\"knitr r\">## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction    A    B    C    D    E\r\n##          A 1116    1    0    0    0\r\n##          B    0  758    3    0    0\r\n##          C    0    0  681    8    0\r\n##          D    0    0    0  635    4\r\n##          E    0    0    0    0  717\r\n## \r\n## Overall Statistics\r\n##                                         \r\n##                Accuracy : 0.996         \r\n##                  95% CI : (0.993, 0.998)\r\n##     No Information Rate : 0.284         \r\n##     P-Value [Acc > NIR] : <2e-16        \r\n##                                         \r\n##                   Kappa : 0.995         \r\n##  Mcnemar's Test P-Value : NA            \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity             1.000    0.999    0.996    0.988    0.994\r\n## Specificity             1.000    0.999    0.998    0.999    1.000\r\n## Pos Pred Value          0.999    0.996    0.988    0.994    1.000\r\n## Neg Pred Value          1.000    1.000    0.999    0.998    0.999\r\n## Prevalence              0.284    0.193    0.174    0.164    0.184\r\n## Detection Rate          0.284    0.193    0.174    0.162    0.183\r\n## Detection Prevalence    0.285    0.194    0.176    0.163    0.183\r\n## Balanced Accuracy       1.000    0.999    0.997    0.993    0.997\r\n</pre></div>\r\n</div></div>\r\n\r\n<p>Looking at rate of error in Figure 1, we determine that we don't need ntrees=100, next time we will use ntrees=70.</p>\r\n<p>\r\nOn the test set, we are seeing <b>>99% accuracy and >99% kappa</b>. This is out of sample error! This is a very deep forest, so let's just plot the importance to understand what's happening in the model.</p>\r\n\r\n<div class=\"chunk\" id=\"unnamed-chunk-3\"><div class=\"rcode\"><div class=\"source\"><pre class=\"knitr r\"><span class=\"hl kwd\">varImpPlot</span><span class=\"hl std\">(rf1)</span>\r\n</pre></div>\r\n</div><div class=\"rimage default\"><img src=\"figure/unnamed-chunk-3.png\" title=\"plot of chunk unnamed-chunk-3\" alt=\"plot of chunk unnamed-chunk-3\" class=\"plot\" /></div></div>\r\n\r\n<p>By far the roll_belt variable is the most important in determining the outcome. So I suppose if you are lifting weights, watch out for \"roll_belt\".</p>\r\n\r\n<p>To be ambitious, let's see if we can improve results with SVM (with regularization c=1 to avoid overfitting) or even stacked modeling.</p>\r\n\r\n<div class=\"chunk\" id=\"unnamed-chunk-4\"><div class=\"rcode\"><div class=\"source\"><pre class=\"knitr r\"><span class=\"hl std\">mod_svm_train</span><span class=\"hl kwb\">&lt;-</span><span class=\"hl kwd\">svm</span><span class=\"hl std\">(class</span><span class=\"hl opt\">~</span><span class=\"hl std\">.,</span><span class=\"hl kwc\">data</span><span class=\"hl std\">=training,</span><span class=\"hl kwc\">cost</span><span class=\"hl std\">=</span><span class=\"hl num\">1</span><span class=\"hl std\">)</span>\r\n<span class=\"hl std\">predictions_train</span><span class=\"hl kwb\">&lt;-</span><span class=\"hl kwd\">predict</span><span class=\"hl std\">(mod_svm_train)</span>\r\n<span class=\"hl kwd\">confusionMatrix</span><span class=\"hl std\">(predictions_train,training</span><span class=\"hl opt\">$</span><span class=\"hl std\">class)</span>\r\n</pre></div>\r\n<div class=\"output\"><pre class=\"knitr r\">## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction    A    B    C    D    E\r\n##          A 4448  200    6    2    0\r\n##          B    7 2783   71    0    6\r\n##          C    8   48 2643  232   59\r\n##          D    0    2   17 2338   50\r\n##          E    1    5    1    1 2771\r\n## \r\n## Overall Statistics\r\n##                                         \r\n##                Accuracy : 0.954         \r\n##                  95% CI : (0.951, 0.958)\r\n##     No Information Rate : 0.284         \r\n##     P-Value [Acc > NIR] : <2e-16        \r\n##                                         \r\n##                   Kappa : 0.942         \r\n##  Mcnemar's Test P-Value : <2e-16        \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity             0.996    0.916    0.965    0.909    0.960\r\n## Specificity             0.981    0.993    0.973    0.995    0.999\r\n## Pos Pred Value          0.955    0.971    0.884    0.971    0.997\r\n## Neg Pred Value          0.999    0.980    0.993    0.982    0.991\r\n## Prevalence              0.284    0.194    0.174    0.164    0.184\r\n## Detection Rate          0.283    0.177    0.168    0.149    0.177\r\n## Detection Prevalence    0.297    0.183    0.190    0.153    0.177\r\n## Balanced Accuracy       0.989    0.955    0.969    0.952    0.980\r\n</pre></div>\r\n<div class=\"source\"><pre class=\"knitr r\"><span class=\"hl std\">predictions_test</span><span class=\"hl kwb\">&lt;-</span><span class=\"hl kwd\">predict</span><span class=\"hl std\">(mod_svm_train,testing)</span>\r\n<span class=\"hl kwd\">confusionMatrix</span><span class=\"hl std\">(predictions_test,testing</span><span class=\"hl opt\">$</span><span class=\"hl std\">class)</span>\r\n</pre></div>\r\n<div class=\"output\"><pre class=\"knitr r\">## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction    A    B    C    D    E\r\n##          A 1107   65    1    1    0\r\n##          B    4  672   26    0    0\r\n##          C    5   22  651   61   13\r\n##          D    0    0    5  580   18\r\n##          E    0    0    1    1  690\r\n## \r\n## Overall Statistics\r\n##                                        \r\n##                Accuracy : 0.943        \r\n##                  95% CI : (0.935, 0.95)\r\n##     No Information Rate : 0.284        \r\n##     P-Value [Acc > NIR] : <2e-16       \r\n##                                        \r\n##                   Kappa : 0.928        \r\n##  Mcnemar's Test P-Value : NA           \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity             0.992    0.885    0.952    0.902    0.957\r\n## Specificity             0.976    0.991    0.969    0.993    0.999\r\n## Pos Pred Value          0.943    0.957    0.866    0.962    0.997\r\n## Neg Pred Value          0.997    0.973    0.990    0.981    0.990\r\n## Prevalence              0.284    0.193    0.174    0.164    0.184\r\n## Detection Rate          0.282    0.171    0.166    0.148    0.176\r\n## Detection Prevalence    0.299    0.179    0.192    0.154    0.176\r\n## Balanced Accuracy       0.984    0.938    0.960    0.948    0.978\r\n</pre></div>\r\n</div></div>\r\n\r\n<p><b>Looks like we have accuracy of >95% and kappa of over 92% in the out of sample set.</b> Not an improvement over random forest on its own. With numbers this high, we would just be tricking ourselves if we started stacking the models. Instead, let's look to see if these 2 models disagree in ANY circumstance for the assignment testing set.</p>\r\n\r\n\r\n<div class=\"chunk\" id=\"unnamed-chunk-5\"><div class=\"rcode\"><div class=\"source\"><pre class=\"knitr r\"><span class=\"hl std\">dat</span><span class=\"hl kwb\">&lt;-</span><span class=\"hl kwd\">read.csv</span><span class=\"hl std\">(</span><span class=\"hl str\">&quot;C:/users/wjc/downloads/pml-testing.csv&quot;</span><span class=\"hl std\">)</span>\r\n<span class=\"hl std\">predictors</span><span class=\"hl kwb\">=</span><span class=\"hl kwd\">cbind</span><span class=\"hl std\">(dat[</span><span class=\"hl num\">8</span><span class=\"hl opt\">:</span><span class=\"hl num\">11</span><span class=\"hl std\">],dat[</span><span class=\"hl num\">37</span><span class=\"hl opt\">:</span><span class=\"hl num\">49</span><span class=\"hl std\">],dat[</span><span class=\"hl num\">60</span><span class=\"hl opt\">:</span><span class=\"hl num\">68</span><span class=\"hl std\">],dat[</span><span class=\"hl num\">84</span><span class=\"hl opt\">:</span><span class=\"hl num\">86</span><span class=\"hl std\">],dat[</span><span class=\"hl num\">102</span><span class=\"hl std\">],dat[</span><span class=\"hl num\">113</span><span class=\"hl opt\">:</span><span class=\"hl num\">124</span><span class=\"hl std\">],dat[</span><span class=\"hl num\">140</span><span class=\"hl std\">],dat[</span><span class=\"hl num\">151</span><span class=\"hl opt\">:</span><span class=\"hl num\">159</span><span class=\"hl std\">])</span>\r\n<span class=\"hl std\">predictions</span><span class=\"hl kwb\">&lt;-</span><span class=\"hl kwd\">predict</span><span class=\"hl std\">(mod_svm_train,predictors)</span>\r\n<span class=\"hl std\">predictions2</span><span class=\"hl kwb\">&lt;-</span><span class=\"hl kwd\">predict</span><span class=\"hl std\">(rf1,predictors)</span>\r\n<span class=\"hl std\">predictions</span><span class=\"hl opt\">==</span><span class=\"hl std\">predictions2</span>\r\n</pre></div>\r\n<div class=\"output\"><pre class=\"knitr r\">##  [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\r\n## [15] TRUE TRUE TRUE TRUE TRUE TRUE\r\n</pre></div>\r\n</div></div>\r\n\r\n<p>Since the resulting predictions for the 20-case test sample are all identical in both models, any stacking or ensembling we do will have no pragmatic impact. After submitting the results, we determine that this algorithm produces 20/20 correct classifications.</p>\r\n\r\n</body>\r\n</html>\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}